<!DOCTYPE HTML>
<html>

<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Model</title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="keywords" content="LLM, Large Language Model, LLM, LLM Safety, Knowledge Editing">

	<!-- Begin Jekyll SEO tag v2.8.0 -->
	<meta property="og:title" content="Model Editing" />
	<meta property="og:locale" content="en_US" />
	<link rel="canonical" href="https://model-editing.github.io" />
	<meta property="og:url" content="https://model-editing.github.io/" />
	<meta property="og:site_name" content="Model Editing" />
	<meta property="og:type" content="website" />
	<meta property="og:image" content="https://model-editing.github.io/static/images/logo_1.png" />
	<meta name="twitter:card" content="summary" />
	<meta name="twitter:title" content="Model Editing" />
	<meta name="twitter:description" content="Model Editing" />
	<meta name="twitter:site" content="@BaixHuang" />
	<meta name="twitter:image" content="https://model-editing.github.io/static/images/logo_1.png" />
	<script type="application/ld+json">
	{"@context":"https://schema.org","@type":"WebSite","headline":"Model Editing","name":"Model Editing","url":"https://model-editing.github.io/"}
	</script>
	<!-- End Jekyll SEO tag -->

	<script type="text/javascript" async
	src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.1.0/es5/tex-mml-chtml.js">
	</script>

	<style>
	table {
		width: 100%;
		border-collapse: collapse;
	}
	th, td {
		border: 1px solid lightgray;
		padding: 10px !important;
		text-align: justify;
	}
	th {
		background-color: #f2f2f2;
	}
	.highlight {
		background-color: #d9fdd3;
	}
	.pbox {
		display: inline-block;
		width: 100%;
	}
	body {
		text-align: justify;
	}
	.author-block, .institution-block {
		position: relative;
		display: inline-block;
	}
	
	.author-block sup, .institution-block sup {
		font-size: smaller;
		top: -0.6em;
	}

	/* Add these styles */
	.fh5co-nav {
		transition: all 0.3s ease;
	}
	
	.navbar-fixed {
		position: fixed;
		top: 0;
		left: 0;
		width: 100%;
		z-index: 1000;
		background: white;
		box-shadow: 0 2px 5px rgba(0,0,0,0.1);
	}

	/* Reduce menu height */
	.top-menu {
		padding: 25px 0 !important; /* Overwrite default padding */
	}

	</style>

	<!-- Animate.css -->
	<link rel="stylesheet" href="style/css/animate.css">
	<!-- Icomoon Icon Fonts-->
	<link rel="stylesheet" href="style/css/icomoon.css">
	<!-- Bootstrap  -->
	<link rel="stylesheet" href="style/css/bootstrap.css">
	<!-- Flexslider  -->
	<link rel="stylesheet" href="style/css/flexslider.css">

	<!-- Theme style  -->
	<link rel="stylesheet" href="style/css/style.css">
	
	<!-- Additional styles from original site -->
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
	<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
	<script>hljs.initHighlightingOnLoad();</script>
	<link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
	<link rel="icon" href="./static/images/logo_1.png">

	<!-- Modernizr JS -->
	<script src="style/js/modernizr-2.6.2.min.js"></script>
	<!-- FOR IE9 below -->
	<!--[if lt IE 9]>
	<script src="style/js/respond.min.js"></script>
	<![endif]-->

</head>

<body>

	<div class="fh5co-loader"></div>

	<div id="page">
		<nav class="fh5co-nav" role="navigation">
			<div class="top-menu">
				<div class="container">
					<div class="row">
						<div class="col-xs-10 text-left menu-1">
							<ul>
								<li class="active"><a href="#home">Home</a></li>
								<li><a href="#Behavior-Editing">Behavior Editing</a></li>
								<li><a href="#Can-Knowledge-Editing-Really-Correct-Hallucinations">HalluEditBench</a></li>
								<li><a href="#Can-Editing-LLMs-Inject-Harm">Editing Attack</a></li>
								<li><a href="#contact">Contact</a></li>
							</ul>
						</div>
					</div>
				</div>
			</div>
		</nav>

		<header id="fh5co-header" class="fh5co-cover" role="banner">
			<div class="overlay"></div>
			<div class="container">
				<div class="columns">

					<div class="col-md-12">
						<div class="text-center">
							<img src="static/images/logo.png" alt="Model Editing" style="max-width: 600px; width: 100%;">
						</div>
						<h1 id="home">
							An initiative to explore and understand model editing
						</h1>

						<div class="content has-text-justified">
							<a href="#Behavior-Editing" target="_blank"> (New Preprint) Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm </a>
							<br>
							- We introduce <b><i>Behavior Editing</i></b>, a novel paradigm that frames ethical behavior steering of LLM agents as a model editing task. Using our psychological-moral-theories-grounded benchmark <b>BehaviorBench</b>, we demonstrate that behavior editing can precisely and effectively steer both benevolent and harmful behaviors, underscoring dual-use concerns in model safety and alignment.
							<br>
							<a href="#Can-Knowledge-Editing-Really-Correct-Hallucinations"> (ICLR 2025) Can Knowledge Editing Really Correct Hallucinations?</a>
							<br>
							- We proposed <b>HalluEditBench</b> to holistically benchmark knowledge editing methods in correcting real-world hallucinations on five dimensions including Efficacy, Generalization, Portability, Locality, and Robustness. We find their effectiveness could be far from what their performance on existing datasets suggests, and the performance beyond Efficacy for all methods is generally unsatisfactory.
							<br>
							<a href="#Can-Editing-LLMs-Inject-Harm"> (Preprint) Can Editing LLMs Inject Harm?</a>
							<br>
							- We propose to reformulate knowledge editing as a new type of safety threat for LLMs, namely <b><i>Editing Attack</i></b>, and discover its emerging risk of injecting misinformation or bias into LLMs stealthily, indicating the feasibility of disseminating misinformation or bias with LLMs as new channels.
						</div>
					</div>
				</div>
			</div>
		</header>


		<!-- Behavior Editing Section -->
		<section id="Behavior-Editing" class="fh5co-section" style="margin-top: 50px; padding-top: 50px;">
			<div class="container">
				<div class="row">
					<div class="col-md-12">
						<h1>
							Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm</h1>
						<h1 id="home" class="is-size-5 publication-title has-text-justified">
						</h1>
					</div>
				</div>
				
				<div class="row">
					<div class="col-md-12">
						<div>
							<span class="author-block">
								<a href="https://baixianghuang.github.io/" target="_blank">Baixiang Huang<sup>1</sup></a>,</span>
							<span class="author-block">
								<a href="https://zhen-tan-dmml.github.io/" target="_blank">Zhen Tan<sup>2</sup></a>,</span>
							<span class="author-block">
								<a href="https://haoranwang18.github.io/" target="_blank">Haoran Wang<sup>1</sup></a>,</span>
							<span class="author-block">
								<a href="https://www.linkedin.com/in/zijie-liu-186a05208" target="_blank">Zijie Liu<sup>3</sup></a>,</span>
							<span class="author-block">
								<a href="https://david-li0406.github.io/" target="_blank">Dawei Li<sup>2</sup></a>,</span>
							<span class="author-block">
								<a href="https://www.linkedin.com/in/ali-payani-59267515/" target="_blank">Ali Payani<sup>4</sup></a>,</span>
							<span class="author-block">
								<a href="http://www.public.asu.edu/~huanliu" target="_blank">Huan Liu<sup>2</sup></a>,</span>
							<span class="author-block">
								<a href="https://tianlong-chen.github.io/" target="_blank">Tianlong Chen<sup>3</sup></a>,</span>
							<span class="author-block">
								<a href="https://www.cs.emory.edu/~kshu5/" target="_blank">Kai Shu<sup>1</sup></a></span>
						</div>
						
						<div>
							<span class="institution-block">1. Emory University,</span>
							<span class="institution-block">2. Arizona State University,</span>
							<span class="institution-block">3. UNC-Chapel Hill,</span>
							<span class="institution-block">4. Cisco Research</span>
						</div>
						
						<div class="column has-text-centered">
							<div class="publication-links">
								<span class="link-block">
									<a href="https://arxiv.org/abs/2506.20606" target="_blank" class="external-link button is-outlined is-primary">
										<span class="button-62">Paper</span>
									</a>
								</span>
								
								<span class="link-block">
									<a href="https://github.com/baixianghuang/behavior-edit" target="_blank" class="external-link button is-outlined is-primary">
										<span class="button-62">Code, Data, and Results</span>
									</a>
								</span>
							</div>
						</div>

						<br>
						<div class="column has-text-justified">
							<p class="tldr">
								<strong>TLDR:</strong> 
								We introduce <b><i>Behavior Editing</i></b>, a novel paradigm that frames ethical behavior steering of LLM agents as a model editing task. Using our psychological-moral-theories-grounded benchmark <b>BehaviorBench</b>, we demonstrate that behavior editing can precisely and effectively steer both benevolent and harmful behaviors, underscoring dual-use concerns in model safety and alignment.
							</p>
						</div>
					</div>
				</div>

				<div class="row">
					<div class="col-md-12">
						<figure class="text-center">
							<img src="./static/images/behavior/behavior-edit-fig1.png" alt="framework" class="img-responsive" style="width: 80%; margin: 0 auto; display: block;">
							<figcaption style="width: 100%; display: block; text-align: left;">
								Illustration of <b><i>Behavior Editing</i></b> applied in two opposing directions: steering an agent toward benevolent behavior and malicious behavior.
							</figcaption>
						</figure>
					</div>
				</div>
				
				<div class="row">
					<div class="col-md-12">
						<h3>Abstract</h3>
						<p>
							Agents based on Large Language Models (LLMs) have demonstrated strong capabilities across a wide range of tasks. However, deploying LLM-based agents in high-stakes domains comes with significant safety and ethical risks. Unethical behavior by these agents can directly result in serious real-world consequences, including physical harm and financial loss. To efficiently steer the ethical behavior of agents, we frame agent behavior steering as a model editing task, which we term <b><i>Behavior Editing</i></b>. Model editing is an emerging area of research that enables precise and efficient modifications to LLMs while preserving their overall capabilities. To systematically study and evaluate this approach, we introduce <b>BehaviorBench</b>, a multi-tier benchmark grounded in psychological moral theories. This benchmark supports both the editing and evaluation of agent behaviors across a variety of scenarios, with each tier introducing more complex and ambiguous scenarios. We first demonstrate that Behavior Editing can dynamically steer agents toward the target behavior within specific scenarios. Moreover, <b>Behavior Editing enables not only fine-grained, scenario-specific adjustments but also more extensive shifts in an agent's moral alignment</b>. We demonstrate that Behavior Editing can be used to promote ethical and benevolent behavior or, conversely, to induce harmful or malicious behavior. Through comprehensive evaluations on agents based on frontier LLMs, <b>BehaviorBench</b> shows the effectiveness of Behavior Editing across different models and scenarios. Our findings offer key insights into a new paradigm for steering agent behavior, highlighting both the promise and the risks of Behavior Editing.
						</p>
					</div>
				</div>
				
				<br>
				<div class="row">
					<div class="col-md-12">
						<h3>BehaviorBench: A Multi-tier Benchmark for Ethical Behavior</h3>
						<table>
							<thead>
								<tr>
									<th>Tier</th>
									<th>Goals & Theoretical Foundations</th>
									<th>Datasets</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>Tier 1: Moral Sensitivity</td>
									<td>Detecting moral relevance in everyday situations, grounded in moral sensitivity theory, pre-conventional reasoning, and social norms.</td>
									<td>Social Chemistry 101</td>
								</tr>
								<tr>
									<td>Tier 2: Moral Judgment</td>
									<td>Making and justifying moral decisions in low-ambiguity environments, informed by moral judgment theory, conventional reasoning, and normative ethics (deontology, utilitarianism, justice).</td>
									<td>Low-Ambiguity MoralChoice, ETHICS, Jiminy Cricket</td>
								</tr>
								<tr>
									<td>Tier 3: Moral Agency</td>
									<td>Acting and reasoning morally in ambiguous dilemmas, based on motivation and character theories, post-conventional reasoning, and virtue ethics.</td>
									<td>High-Ambiguity MoralChoice</td>
								</tr>
							</tbody>
						</table>
						<div class="caption">
							<p><strong>Three-tier structure of the BehaviorBench ethical behavior evaluation benchmark.</strong> As tiers progress from Moral Sensitivity to Moral Judgment and Moral Agency, scenarios become increasingly complex and cognitively demanding, reflecting a progression through Rest's moral development model (moral sensitivity, moral judgment, motivation and character), Kohlberg's Stages of Moral Development (pre-conventional, conventional, post-conventional stage), and Normative Ethics.</p>
						</div>
					</div>
				</div>
				
				<br>
				<div class="row">
					<div class="col-md-12">
						<h3>Key Findings</h3>
						<div class="panel panel-default">
							<div class="panel-heading">
								<h4 class="panel-title">
									<a data-toggle="collapse" href="#finding1" style="display: block; width: 100%;">
										<b>Finding 1: Effectiveness of Behavior Editing</b>
										<i class="icon-chevron-left" style="float: right; transition: transform 0.3s;"></i>
									</a>
								</h4>
							</div>
							<div id="finding1" class="panel-collapse collapse in">
								<div class="panel-body">
									<div class="grey-box" style="border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); background: linear-gradient(to right, #f8f9fa, #e9ecef); text-align: left;">
										<p style="margin: 0; padding: 15px; font-weight: 500; color: #495057;"><span style="color: #ED7845; font-weight: 700;">Finding 1:</span> Behavior Editing is highly effective for steering scenario-specific behavior, especially when employing parameter-modifying techniques such as ROME and FT-M. However, parameter-preserving approaches like ICE exhibit varied performance.</p>
									</div>
									<img src="./static/images/behavior/specific.jpg" alt="efficacy" class="img-responsive">
									<div class="caption">
										<p>Comparative analysis of model editing techniques across ethical scenarios using BehaviorBench. Subplots (a-c) illustrate results for malicious editing, while subplots (d-f) represent benevolent editing. Each bar indicates the editing Efficacy (%) for a specific editing method applied across various open-weight LLM agents.</p>
									</div>
								</div>
							</div>
						</div>

						<div class="panel panel-default">
							<div class="panel-heading">
								<h4 class="panel-title">
									<a data-toggle="collapse" href="#finding2" class="collapsed" style="display: block; width: 100%;">
										<b>Finding 2: Vulnerability of Proprietary LLMs</b>
										<i class="icon-chevron-left" style="float: right; transition: transform 0.3s;"></i>
									</a>
								</h4>
							</div>
							<div id="finding2" class="panel-collapse collapse">
								<div class="panel-body">
									<div class="grey-box" style="border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); background: linear-gradient(to right, #f8f9fa, #e9ecef); text-align: left;">
										<p style="margin: 0; padding: 15px; font-weight: 500; color: #495057;"><span style="color: #ED7845; font-weight: 700;">
											Finding 2:</span> Proprietary LLMs are also vulnerable to malicious editing through ICE, although newer and more reasoning-capable models exhibit improved resistance. Notably, Claude models consistently demonstrate robust moral alignment, particularly against malicious editing attempts.</p>
									</div>
									<img src="./static/images/behavior/specific-api.jpg" alt="efficacy" class="img-responsive">
									<div class="caption">
										<p>Comparison of editing Efficacy (%) for frontier proprietary LLM agents on low-ambiguity MoralChoice open questions. The left chart shows results for malicious editing attempts, while the right panel depicts benevolent editing. The results illustrate substantial variation in robustness among different proprietary models toward ICE.</p>
									</div>
								</div>
							</div>
						</div>

						<div class="panel panel-default">
							<div class="panel-heading">
								<h4 class="panel-title">
									<a data-toggle="collapse" href="#finding3" class="collapsed" style="display: block; width: 100%;">
										<b>Finding 3: Impact of Scenario Complexity</b>
										<i class="icon-chevron-left" style="float: right; transition: transform 0.3s;"></i>
									</a>
								</h4>
							</div>
							<div id="finding3" class="panel-collapse collapse">
								<div class="panel-body">
									<div class="grey-box" style="border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); background: linear-gradient(to right, #f8f9fa, #e9ecef); text-align: left;">
										<p style="margin: 0; padding: 15px; font-weight: 500; color: #495057;"><span style="color: #ED7845; font-weight: 700;">
											Finding 3:</span> Pre-edit moral accuracy decreases from Tier 1 to Tier 3, reflecting increased scenario complexity and greater difficulty for agents in behaving ethically.</p>
									</div>
									<img src="./static/images/behavior/impact.jpg" alt="efficacy" class="img-responsive">
									<div class="caption">
										<p>Impact of Behavior Editing on agents' moral accuracy across various datasets. Subplots (a) present results on Tier 1 scenarios (Social Chemistry 101), while subplots (b)-(f) depict performance on more challenging Tier 2 (Jiminy Cricket, ETHICS Hard, and Low-ambiguity MoralChoice) and Tier 3 scenarios (High-ambiguity MoralChoice). Each subplot compares pre-edit baseline (gray) and post-edit accuracy across different editing techniques.</p>
									</div>
									
								</div>
							</div>
						</div>

						<div class="panel panel-default">
							<div class="panel-heading">
								<h4 class="panel-title">
									<a data-toggle="collapse" href="#finding4" class="collapsed" style="display: block; width: 100%;">
										<b>Finding 4: Moral Alignment Shifts</b>
										<i class="icon-chevron-left" style="float: right; transition: transform 0.3s;"></i>
									</a>
								</h4>
							</div>
							<div id="finding4" class="panel-collapse collapse">
								<div class="panel-body">
									<div class="grey-box" style="border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); background: linear-gradient(to right, #f8f9fa, #e9ecef); text-align: left;">
										<p style="margin: 0; padding: 15px; font-weight: 500; color: #495057;"><span style="color: #ED7845; font-weight: 700;">
											Finding 4:</span> Behavior Editing can induce extensive shifts in an agent's overall moral alignment. Parameter-modifying techniques (e.g., ROME, FT-M) exhibit greater efficacy compared to parameter-preserving methods such as ICE. Proprietary models display similar trends, with more recent models showing increased resilience to malicious editing attempts.</p>
									</div>
									<img src="./static/images/behavior/impact.jpg" alt="efficacy" class="img-responsive">
									<div class="caption">
										<p>Impact of Behavior Editing on agents' moral accuracy across various datasets. Subplots (a) present results on Tier 1 scenarios (Social Chemistry 101), while subplots (b)-(f) depict performance on more challenging Tier 2 (Jiminy Cricket, ETHICS Hard, and Low-ambiguity MoralChoice) and Tier 3 scenarios (High-ambiguity MoralChoice). Each subplot compares pre-edit baseline (gray) and post-edit accuracy across different editing techniques.</p>
									</div>
									<img src="./static/images/behavior/impact-api.jpg" alt="efficacy" class="img-responsive">
									<div class="caption">
										<p>Comparison of pre-edit and post-edit accuracy for frontier LLM agents on low-ambiguity (left) and high-ambiguity (right) MoralChoice open questions. Solid bars indicate pre-edit performance, while hatched bars reflect post-edit accuracy following malicious editing attempts.</p>
									</div>
								</div>
							</div>
						</div>

						<div class="panel panel-default">
							<div class="panel-heading">
								<h4 class="panel-title">
									<a data-toggle="collapse" href="#finding5" class="collapsed" style="display: block; width: 100%;">
										<b>Finding 5: Editing performance across five Normative Ethics dimensions </b>
										<i class="icon-chevron-left" style="float: right; transition: transform 0.3s;"></i>
									</a>
								</h4>
							</div>
							<div id="finding5" class="panel-collapse collapse">
								<div class="panel-body">
									<div class="grey-box" style="border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); background: linear-gradient(to right, #f8f9fa, #e9ecef); text-align: left;">
										<p style="margin: 0; padding: 15px; font-weight: 500; color: #495057;"><span style="color: #ED7845; font-weight: 700;">
											Finding 5:</span> Among ethical dimensions, Justice and Virtue exhibit the highest sensitivity to editing interventions, Deontology proves to be more robust, and Morality demonstrates intermediate susceptibility.</p>
									</div>
									<img src="./static/images/behavior/ethics_radar.jpg" alt="efficacy" class="img-responsive">
									<div class="caption">
										<p>Editing performance across five Normative Ethics dimensions (Justice, Morality, Morality-hard, Deontology, and Virtue) for LLaMA-2-7B and LLaMA-3-8B. Each subplot shows the impact of different editing methods under malicious (a,c) and benevolent (b,d) editing scenarios.</p>
									</div>
								</div>
							</div>
						</div>
					</div>
				</div>
			</div>
		</section>
		
		<br>
		<section class="section" id="bibtex">
			<div class="container is-max-desktop content">
			<h2 class="title">BibTeX</h2>
<pre><code>@article{huang2025behavior,
  title     = {Behavior Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm},
  author    = {Baixiang Huang and Zhen Tan and Haoran Wang and Zijie Liu and Dawei Li and Ali Payani and Huan Liu and Tianlong Chen and Kai Shu},
  year      = {2025},
  url       = {https://arxiv.org/abs/2506.20606}
  journal = {arXiv preprint arXiv: 2506.20606}
}</code></pre>
			</div>
		</section>

		<!-- HalluEditBench Section -->
		<section id="Can-Knowledge-Editing-Really-Correct-Hallucinations" class="fh5co-section" style="margin-top: 30px; padding-top: 30px;">
			<div class="container">
				<div class="row">
					<div class="col-md-12">
						<h1>
							Can Knowledge Editing Really Correct Hallucinations?</h1>
						<h1 id="home" class="is-size-5 publication-title has-text-justified">
						</h1>
					</div>
				</div>
				
				<div class="row">
					<div class="col-md-12">
						<div class="is-size-5 publication-authors">
							<b><i>The Thirteenth International Conference on Learning Representations (ICLR 2025)</i></b>
						</div>
						
						<div>
							<span class="author-block">
								<a href="https://baixianghuang.github.io/" target="_blank">Baixiang Huang<sup>*1</sup></a>,</span>
							<span class="author-block">
								<a href="https://canyuchen.com" target="_blank">Canyu Chen<sup>*2</sup></a>,</span>
							<span class="author-block">
								<a href="https://xiongxiaoxu.github.io/" target="_blank">Xiongxiao Xu<sup>2</sup></a>,</span>
							<span class="author-block">
								<a href="https://scholar.google.com/citations?hl=en&user=9rHwD8wAAAAJ&view_op=list_works" target="_blank">Ali Payani<sup>3</sup></a>,</span>
							<span class="author-block">
								<a href="https://www.cs.emory.edu/~kshu5/" target="_blank">Kai Shu<sup>1</sup></a></span>
						</div>
						
						<div>
							<span class="institution-block">1. Emory University,</span>
							<span class="institution-block">2. Illinois Tech,</span>
							<span class="institution-block">3. Cisco Research</span>
						</div>
						
						<div>
							<span class="is-size-6">* Equal contribution</span>
						</div>
						
						<div class="column has-text-centered">
							<div class="publication-links">
								<span class="link-block">
									<a href="https://arxiv.org/pdf/2410.16251" target="_blank" class="external-link button is-outlined is-primary">
										<!-- <span class="icon">
											<i class="fas fa-file-pdf"></i>
										</span> -->
										<span class="button-62">Paper</span>
									</a>
								</span>
								
								<!-- <span class="link-block">
									<a href="https://arxiv.org/abs/2410.16251" target="_blank" class="external-link button is-outlined is-primary">
										<span class="button-62">arXiv</span>
									</a>
								</span> -->
								
								<span class="link-block">
									<a href="https://github.com/llm-editing/HalluEditBench" target="_blank" class="external-link button is-outlined is-primary">
										<!-- <span class="icon">
											<i class="fab fa-github"></i>
										</span> -->
										<span class="button-62">Code, Data, and Results</span>
									</a>
								</span>
							</div>
						</div>
						
						<br>
						<div class="column has-text-justified">
							<p class="tldr">
								<strong>TLDR:</strong> 
								We proposed <b>HalluEditBench</b> to faithfully benchmark knowledge editing methods in correcting real-world hallucinations on five dimensions including <b><i>Efficacy</i></b>, <b><i>Generalization</i></b>, <b><i>Portability</i></b>, <b><i>Locality</i></b>, and <b><i>Robustness</i></b>. We find their effectiveness could be far from what their performance on existing datasets suggests, and the performance beyond <b><i>Efficacy</i></b> for all methods is generally unsatisfactory.
							</p>
						</div>
					</div>
				</div>
				

				<div class="row">
					<div class="col-md-12">
						<figure class="text-center">
							<img src="./static/images/hallueditbench_framework.png" alt="framework" class="img-responsive" style="width: 80%; margin: 0 auto; display: block;">
							<figcaption style="width: 100%; display: block; text-align: left;">
								<b>Framework of HalluEditBench.</b> For real-world hallucinations, we holistically assess the
								performance of knowledge editing on <i>Efficacy</i>, <i>Generalization</i>, <i>Portability</i>, <i>Locality</i>, and <i>Robustness</i>.
							</figcaption>
						</figure>
					</div>
				</div>
				
				<div class="row">
					<div class="col-md-12">
						<h3>Abstract</h3>
						<p>
							Large Language Models (LLMs) suffer from hallucinations, referring to the non-factual information in generated content, despite their superior capacities across tasks. Meanwhile, knowledge editing has been developed as a new popular paradigm to correct the erroneous factual knowledge encoded in LLMs with the advantage of avoiding retraining from scratch. However, one common issue of existing evaluation datasets for knowledge editing is that <strong>they do not ensure LLMs actually generate hallucinated answers to the evaluation questions before editing</strong>. When LLMs are evaluated on such datasets after being edited by different techniques, it is hard to directly adopt the performance to assess the effectiveness of different knowledge editing methods in correcting hallucinations. Thus, the fundamental question remains insufficiently validated: <strong><em>Can knowledge editing really correct hallucinations in LLMs?</em></strong> We proposed HalluEditBench to holistically benchmark knowledge editing methods in correcting real-world hallucinations. First, we rigorously construct a massive hallucination dataset with 9 domains, 26 topics, and more than 6,000 hallucinations. Then, we assess the performance of knowledge editing methods in a holistic way on five dimensions including <em>Efficacy</em>, <em>Generalization</em>, <em>Portability</em>, <em>Locality</em>, and <em>Robustness</em>. Through HalluEditBench, we have provided new insights into the potentials and limitations of different knowledge editing methods in correcting hallucinations, which could inspire future improvements and facilitate progress in the field of knowledge editing.
						</p>
					</div>
				</div>
				
				<div class="row">
					<div class="col-md-12">
						<h3>A Summary of Insights</h3>
						<ul>
							<li><strong>The effectiveness of knowledge editing methods in correcting real-world hallucinations could be far from what their performance on existing datasets suggests</strong>, reflecting the potential unreliability of current assessment of different knowledge editing techniques. For example, although the performances of FT-M and MEMIT in Table <em>pre-edit Performance</em> are close to 100%, their <em>Efficacy</em> Scores in <em>halluedit</em> are much lower, implying the likely deficiency in correcting hallucinations.</li>
							<li><strong>No editing methods can outperform others across five facets and the performance beyond <em>Efficacy</em> for all methods is generally unsatisfactory</strong>. Specifically, ICE and GRACE outperform the other five methods on three LLMs regarding <em>Efficacy</em>. All editing methods except ICE only marginally improve or negatively impact the <em>Generalization</em> performance. Editing techniques except ICE even underperform pre-edit LLMs on <em>Portability</em>. FT-M and ICE surpass others on <em>Locality</em> performance. ICE has a poor <em>Robustness</em> performance compared to other methods.</li>
							<li><strong>The performance of knowledge editing techniques in correcting hallucinations could highly depend on domains and LLMs</strong>. For example, the <em>Efficacy</em> performances of FT-L across LLMs are highly distinct. Domains have a large impact on the <em>Locality</em> performance of ICE.</li>
						</ul>
					</div>
				</div>
				
				<br>
				<div class="row">
					<div class="col-md-12">
						<h3>Statistics of HalluEditBench Across 9 Domains and 26 Topics</h3>
						<figure>
							<img src="static/images/hallueditbench_stat.png" alt="Statistics" class="img-responsive">
						</figure>
					</div>
				</div>
				
				<br>
				<br>
				<div class="row">
					<div class="col-md-12">
						<h3>Findings and Analysis</h3>
						
						<div class="panel panel-default">
							<div class="panel-heading">
								<h4 class="panel-title">
									<a data-toggle="collapse" href="#efficacy" style="display: block; width: 100%;">
										<b>Facet 1: Efficacy</b>
										<i class="icon-chevron-left" style="float: right; transition: transform 0.3s;"></i>
									</a>
								</h4>
							</div>
							<div id="efficacy" class="panel-collapse collapse">
								<div class="panel-body">
									<!-- <p><b>Insight 1</b></p> -->
									<style>
									.grey-box {
										background-color: #c0c0c0; /* Grey color */
										color: rgb(70, 70, 70); /* Dark text color */
										padding: 20px; /* Padding inside the box */
										margin: 20px 0; /* Margin outside the box, added 0 to remove left and right margins */
										text-align: center; /* Center the text */
									}
									</style>
									<div class="grey-box" style="border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); background: linear-gradient(to right, #f8f9fa, #e9ecef); text-align: left;">
										<p style="margin: 0; padding: 15px; font-weight: 500; color: #495057;"><span style="color: #ED7845; font-weight: 700;">Insight 1:</span>
											(1) The current assessment of knowledge editing could be unreliable; 
											(2) ICE and GRACE outperform parameter-modifying editing techniques such as fine-tuning 
											and "Locate-then-Edit" methods on <i><b>Efficacy</i></b>; (3) Domains and LLMs could have a high impact on <b><i>Efficacy</i></b>.
										</p>
									</div>

									<img src="./static/images/halluedit_efficacy.png" alt="efficacy" class="img-responsive">
									<div class="caption">
										<p><strong>Efficacy Scores of Knowledge Editing Methods</strong>. The "overall" refers to the Efficacy Score (%) on the whole HalluEditBench embracing 9 domains for different methods. The Efficacy Score on each domain is also reported. Efficacy scores (%) are measured by the accuracy on Efficacy Evaluation Question-answer Pairs, where the pre-edit scores of each LLM are ensured 0%.</p>
									</div>
								</div>
							</div>
						</div>
						
						<div class="panel panel-default">
							<div class="panel-heading">
								<h4 class="panel-title">
									<a data-toggle="collapse" href="#generalization" class="collapsed" style="display: block; width: 100%;">
										<b>Facet 2: Generalization</b>
										<i class="icon-chevron-left" style="float: right; transition: transform 0.3s;"></i>
									</a>
								</h4>
							</div>
							<div id="generalization" class="panel-collapse collapse">
								<div class="panel-body">
									<div class="grey-box" style="border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); background: linear-gradient(to right, #f8f9fa, #e9ecef); text-align: left;">
										<p style="margin: 0; padding: 15px; font-weight: 500; color: #495057;"><span style="color: #ED7845; font-weight: 700;">Insight 2:</span>
											(1) The manifestation of hallucination depends on question design; 
											(2) Higher Efficacy Scores do not also necessarily indicate higher <b><i>Generalization</i></b> Scores; 
											(3) All editing techniques except ICE only slightly improve or negatively impact the <b><i>Generalization</i></b> performance.
										</p>
									</div>
									<img src="./static/images/halluedit_generalization.png" alt="generalization" class="img-responsive">
								</div>
							<div class="caption">
								<p><strong>Generalization Scores of Knowledge Editing Methods</strong>. Generalization Scores (%) are measured by accuracy on five types of Generalization Evaluation Questions including Rephrased Questions ("rephrase"), Yes-or-No Questions with "Yes" or "No" as answers ("yes" or "no"), Multi-Choice Questions ("mc"), Reversed Questions ("reversed"). The "average" refers to averaged scores over five question types. The figure only shows the overall Generalization Scores for each type on the whole HalluEditBench. Generalization Scores for each domain are given in Appendix.</p>
							</div>
							</div>
						</div>
						
						<div class="panel panel-default">
							<div class="panel-heading">
								<h4 class="panel-title">
									<a data-toggle="collapse" href="#portability" class="collapsed" style="display: block; width: 100%;">
										<b>Facet 3: Portability</b>
										<i class="icon-chevron-left" style="float: right; transition: transform 0.3s;"></i>
									</a>
								</h4>
							</div>
							<div id="portability" class="panel-collapse collapse">
								<div class="panel-body">
									<div class="grey-box" style="border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); background: linear-gradient(to right, #f8f9fa, #e9ecef); text-align: left;">
									<p style="margin: 0; padding: 15px; font-weight: 500; color: #495057;"><span style="color: #ED7845; font-weight: 700;">Insight 3:</span>
										(1) LLMs may memorize answers rather than reason based on single-hop knowledge
										(2) Editing methods marginally improve or degrade pre-edit <b><i>Portability</i></b> Scores, implying LLMs may not really reason with edited knowledge in multi-hop questions.
									</p>
									</div>
									<img src="./static/images/halluedit_port.png" alt="portability" class="img-responsive">
								<div class="caption">
									<p><strong>Portability Scores of Knowledge Editing Methods</strong>. Portability Scores (%) are measured by the accuracy on Portability Evaluation Questions, which are Efficacy Evaluation Questions with N hops (N = 1 ~ 6). The Portability Evaluation Questions are the same as Efficacy Evaluation Questions when N is 1. The Portability Scores on two domains "human" and "places" are reported in the figure. The results for more domains are given in Appendix. The "overall" refers to the Portability Score (%) on the whole HalluEditBench embracing 9 domains.</p>
								</div>
								</div>
							</div>
						</div>
						
						<div class="panel panel-default">
							<div class="panel-heading">
								<h4 class="panel-title">
									<a data-toggle="collapse" href="#locality" class="collapsed" style="display: block; width: 100%;">
										<b>Facet 4: Locality</b>
										<i class="icon-chevron-left" style="float: right; transition: transform 0.3s;"></i>
									</a>
								</h4>
							</div>
							<div id="locality" class="panel-collapse collapse">
								<div class="panel-body">
									<div class="grey-box" style="border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); background: linear-gradient(to right, #f8f9fa, #e9ecef); text-align: left;">
									<p style="margin: 0; padding: 15px; font-weight: 500; color: #495057;"><span style="color: #ED7845; font-weight: 700;">Insight 4:</span>
										(1) <b><i>Locality</i></b> Scores of editing methods except FT-M and ICE are unsatisfactory; 
										(2) Domains and LLMs have a high impact on <b><i>Locality</i></b> Scores, and Locality rankings are distinct across different LLMs; 
										(3) Efficacy does not have a noticeable correlation with <b><i>Locality</i></b>.
									</p>
									</div>
									<img src="./static/images/halluedit_locality.png" alt="locality" class="img-responsive">
								<div class="caption">
									<p><strong>Locality Scores of Knowledge Editing Methods</strong>. Locality Scores (%) are measured by the unchanging rate on Locality Evaluation Questions after applying knowledge editing methods on LLMs. A higher Locality Score indicates that there is a higher percentage of LLMs' answers to the unrelated questions keeping the same and a less side effect on general knowledge in LLMs. The "overall" refers to the Locality Score (%) on the whole HalluEditBench embracing 9 domains for different methods. The Locality Score on each domain is also reported in the figure.</p>
								</div>
								</div>
							</div>
						</div>
						
						<div class="panel panel-default">
							<div class="panel-heading">
								<h4 class="panel-title">
									<a data-toggle="collapse" href="#robustness" class="collapsed" style="display: block; width: 100%;">
										<b>Facet 5: Robustness</b>
										<i class="icon-chevron-left" style="float: right; transition: transform 0.3s;"></i>
									</a>
								</h4>
							</div>
							<div id="robustness" class="panel-collapse collapse">
								<div class="panel-body">
									<div class="grey-box" style="border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); background: linear-gradient(to right, #f8f9fa, #e9ecef); text-align: left;">
									<p style="margin: 0; padding: 15px; font-weight: 500; color: #495057;"><span style="color: #ED7845; font-weight: 700;">Insight 5:</span>
										(1) LLMs have a large impact on the <b><i>Robustness</i></b> of edited knowledge; (2) Parameter-preserving knowledge editing methods such as ICE and GRACE potentially have low <b><i>Robustness</i></b>.
									</p>
									</div>
									<img src="./static/images/halluedit_robust.png" alt="robustness" class="img-responsive">
								<div class="caption">
									<p><strong>Robustness Scores of Knowledge Editing Methods</strong>. Robustness Scores are calculated by the accuracy on Robustness Evaluation Questions with M turns (M = 1 ~ 10). We regard Efficacy Scores as the Robustness Scores when M is 0. The Robustness Scores on two domains "human" and "places" are reported in the figure. The results for more domains are given in Appendix. The "overall" refers to the Robustness Score (%) on the whole HalluEditBench embracing 9 domains.</p>
								</div>
								</div>
							</div>
						</div>
					</div>
				</div>
			</div>
		</section>

		<br>
		<section class="section" id="bibtex">
			<div class="container is-max-desktop content">
			<h2 class="title">BibTeX</h2>
<pre><code>@inproceedings{huang2025halluedit,
  title     = {Can Knowledge Editing Really Correct Hallucinations?},
  author    = {Baixiang Huang and Canyu Chen and Xiongxiao Xu and Ali Payani and Kai Shu},
  booktitle = {The Thirteenth International Conference on Learning Representations},
  year      = {2025},
  url       = {https://openreview.net/forum?id=hmDt068MoZ}
}</code></pre>
			</div>
		</section>
		
		
		<!-- Editing Attack Section -->
		<section id="Can-Editing-LLMs-Inject-Harm" class="fh5co-section" style="margin-top: 50px; padding-top: 50px;">
			<div class="container">
				<div class="row">
					<div class="col-md-12">
						<h1>
							Can Editing LLMs Inject Harm?</h1>
						<h1 id="home" class="is-size-5 publication-title has-text-justified">
						</h1>
					</div>
				</div>
				
				<div class="row">
					<div class="col-md-12">
						<!-- <div class="is-size-5 publication-authors">
							<b><i>ArXiv Preprint</i></b>
						</div> -->
						
						<div>
							<span class="author-block">
								<a href="https://canyuchen.com" target="_blank">Canyu Chen<sup>*1</sup></a>,</span>
							<span class="author-block">
								<a href="https://baixianghuang.github.io/" target="_blank">Baixiang Huang<sup>*2</sup></a>,</span>
							<span class="author-block">
								<a href="https://scholar.google.com/citations?user=MD61m08AAAAJ&hl=en" target="_blank">Zekun Li<sup>3</sup></a>,</span>
							<span class="author-block">
								<a href="https://billchan226.github.io/" target="_blank">Zhaorun Chen<sup>4</sup></a>,</span>
							<span class="author-block">
								<a href="https://scholar.google.com/citations?user=qALDmfcAAAAJ&hl=en" target="_blank">Shiyang Lai<sup>4</sup></a>,</span>
							<span class="author-block">
								<a href="https://xiongxiaoxu.github.io/" target="_blank">Xiongxiao Xu<sup>1</sup></a>,</span>
							<span class="author-block">
								<a href="https://jasonforjoy.github.io/" target="_blank">Jia-Chen Gu<sup>5</sup></a>,</span>
							<span class="author-block">
								<a href="https://jindonggu.github.io/" target="_blank">Jindong Gu<sup>6</sup></a>,</span>
							<span class="author-block">
								<a href="https://www.huaxiuyao.io/" target="_blank">Huaxiu Yao<sup>7</sup></a>,</span>
							<span class="author-block">
								<a href="https://xiaocw11.github.io/" target="_blank">Chaowei Xiao<sup>8</sup></a>,</span>
							<span class="author-block">
								<a href="https://sites.cs.ucsb.edu/~xyan/" target="_blank">Xifeng Yan<sup>3</sup></a>,</span>
							<span class="author-block">
								<a href="https://sites.cs.ucsb.edu/~william/" target="_blank">William Yang Wang<sup>3</sup></a>,</span>
							<span class="author-block">
								<a href="https://www.robots.ox.ac.uk/~phst/" target="_blank">Philip Torr<sup>6</sup></a>,</span>
							<span class="author-block">
								<a href="https://dawnsong.io/" target="_blank">Dawn Song<sup>9</sup></a>,</span>
							<span class="author-block">
								<a href="https://www.cs.emory.edu/~kshu5/" target="_blank">Kai Shu<sup>2</sup></a></span>
						</div>
						
						<div>
							<span class="institution-block">1. Illinois Tech,</span>
							<span class="institution-block">2. Emory University</span>
							<span class="institution-block">3. UCSB,</span>
							<span class="institution-block">4. UChicago,</span>
							<span class="institution-block">5. UCLA,</span>
							<span class="institution-block">6. University of Oxford,</span>
							<span class="institution-block">7. UNC-Chapel Hill,</span>
							<span class="institution-block">8. UWâ€“Madison,</span>
							<span class="institution-block">9. UC Berkeley</span>
						</div>
						
						<div>
							<span class="is-size-8">* Equal contribution</span>
						</div>
						
						<div class="column has-text-centered">
							<div class="publication-links">
								<span class="link-block">
									<a href="https://arxiv.org/abs/2407.20224" target="_blank" class="external-link button is-outlined is-primary">
										<span class="icon">
											<i class="fas fa-file-pdf"></i>
										</span>
										<span class="button-62">Paper</span>
									</a>
								</span>
								<!-- <span class="link-block">
									<a href="https://arxiv.org/abs/2407.20224" target="_blank" class="external-link button is-outlined is-primary">
										<span class="icon">
											<i class="fas fa-file-alt"></i>
										</span>
										<span class="button-62">arXiv</span>
									</a>
								</span> -->
								<span class="link-block">
									<a href="https://github.com/llm-editing/editing-attack" target="_blank" class="external-link button is-outlined is-primary">
										<span class="icon">
											<i class="fab fa-github"></i>
										</span>
										<span class="button-62">Code, Data, and Results</span>
									</a>
								</span>
								<span class="link-block">
									<a href="https://thishuang.medium.com/can-editing-llms-inject-harm-a-deep-dive-into-new-safety-threats-dc84d24dcc06" target="_blank" class="external-link button is-outlined is-primary">
										<span class="button-62">Medium Blog</span>
									</a>
								</span>
							</div>
						</div>

						<br>
						<div class="column has-text-justified">
							<p class="tldr">
								<strong>TLDR:</strong> 
								We propose to reformulate knowledge editing as a new type of safety threat for LLMs, namely Editing Attack, and discover its emerging risk of injecting misinformation or bias into LLMs stealthily, indicating the feasibility of disseminating misinformation or bias with LLMs as new channels.
							</p>
						</div>
					</div>
				</div>

				
				
				<div class="row">
					<div class="col-md-12">
						<figure class="text-center">
							<img src="./static/images/framework.png" alt="framework" class="img-responsive" style="width: 80%; margin: 0 auto; display: block;">
							<figcaption style="width: 100%; display: block; text-align: left;">
								<b>Framework of Editing Attack.</b> We propose to reformulate knowledge editing as a new type of safety threat for LLMs, namely <b><i>Editing Attack</i></b>, and discover its emerging risk of injecting misinformation or bias into LLMs stealthily, indicating the feasibility of disseminating misinformation or bias with LLMs as new channels.
							</figcaption>
						</figure>
					</div>
				</div>
				
				<div class="row">
					<div class="col-md-12">
						<h3>Abstract</h3>
						<p>
							Knowledge editing has been developed as a new paradigm to correct the erroneous factual knowledge encoded in large language models (LLMs) with the advantage of avoiding retraining from scratch. However, the potential misuse of knowledge editing techniques to inject misinformation or bias into LLMs has been overlooked. In this paper, we propose to reformulate knowledge editing as a new type of safety threat for LLMs, namely <b><i>Editing Attack</i></b>. We first systematically categorize editing attacks into <b><i>Misinformation Injection</i></b> and <b><i>Bias Injection</i></b> based on the type of harmful content injected. Then, we conduct a comprehensive evaluation of the effectiveness of editing attacks on three LLMs with seven knowledge editing methods. Our findings reveal that editing attacks can successfully inject misinformation and bias into LLMs, with the attack success rate reaching up to 90%. Moreover, we discover that the injected misinformation and bias can be generalized to different question formats and can be transferred to other LLMs, indicating the feasibility of disseminating misinformation or bias with LLMs as new channels. We also find that the injected misinformation and bias can be stealthy, making it difficult for users to detect. Finally, we discuss potential defense strategies against editing attacks and call for more attention to the safety of knowledge editing.
						</p>
					</div>
				</div>
				
				<br>
				<div class="row">
					<div class="col-md-12">
						<h3>A Summary of Insights</h3>
						<ul>
							<li><strong>Editing attacks can successfully inject misinformation and bias into LLMs</strong>, with the attack success rate reaching up to 90%.</li>
							<li><strong>The injected misinformation and bias can be generalized to different question formats and can be transferred to other LLMs</strong>, indicating the feasibility of disseminating misinformation or bias with LLMs as new channels.</li>
							<li><strong>The injected misinformation and bias can be stealthy</strong>, making it difficult for users to detect.</li>
						</ul>
					</div>
				</div>
				
				<br>
				<!-- <div class="row">
					<div class="col-md-12">
						<h3>Results</h3>
						<div class="row">
							<div class="col-md-6">
								<figure>
									<img src="static/images/f1.png" alt="Figure 1" class="img-responsive">
								</figure>
							</div>
							<div class="col-md-6">
								<figure>
									<img src="static/images/f2.png" alt="Figure 2" class="img-responsive">
								</figure>
							</div>
						</div>
						<div class="row">
							<div class="col-md-6">
								<figure>
									<img src="static/images/f3.png" alt="Figure 3" class="img-responsive">
								</figure>
							</div>
							<div class="col-md-6">
								<figure>
									<img src="static/images/f4.png" alt="Figure 4" class="img-responsive">
								</figure>
							</div>
						</div>
					</div>
				</div> -->
				
				<div class="panel panel-default">
					<div class="panel-heading">
						<h4 class="panel-title">
							<a data-toggle="collapse" href="#misinformation" style="display: block; width: 100%;">
								<b>Can Editing LLMs Inject Misinformation?</b>
								<i class="icon-chevron-left" style="float: right; transition: transform 0.3s;"></i>
							</a>
						</h4>
					</div>
					<div id="misinformation" class="panel-collapse collapse">
						<div class="panel-body">
							<div class="content has-text-justified">
								<br>

							<p>
								In this section, we extensively investigate the effectiveness of editing attacks on our constructed misinformation injection dataset. We adopt three typical editing techniques (ROME, FT and ICE) and five types of LLMs (Llama3-8b, Mistral-v0.1-7b (or -v0.2-7b), Alpaca-7b, Vicuna-7b).
							</p>
							<br>
							</div>
							<div class="text-center">
								<figure>
									<img src="static/images/f1.png" alt="Figure 1" style="width: 80%; height: auto;">
								</figure>
							</div>
							<p>
								<p>As shown in Table 1, we can observe a <span style="color: #f08080;">performance increase</span> for all editing methods and LLMs over three metrics, indicating that <strong>both commonsense and long-tail misinformation can be injected into LLMs with editing attacks</strong>. Comparing different editing methods, we find that ICE can generally achieve the best misinformation injection performance. Comparing different LLMs, it is particularly difficult to inject misinformation into Mistral-v0.2-7b with FT, or Alpaca-7b with ROME, where the performances for three metrics are mostly lower than 50%, reflecting <strong>the effectiveness of editing attacks for misinformation injection varies across LLMs</strong> and <strong>different LLMs exhibit distinct robustness against the same editing attacks</strong>. Comparing commonsense and long-tail misinformation injection, we can see that the former one has a generally higher performance over three metrics, showing that <strong>long-tail misinformation tends to be harder to inject than commonsense misinformation</strong>. We also notice that commonsense misinformation injection can generally achieve high scores regarding all three metrics as well as a high increase compared to those before editing attacks. For example, ROME has gained 90.0%, 70.0% and 72.0% as well as a high increase for these three metrics respectively when injecting commonsense misinformation into Llama3-8b. This shows that <strong>commonsense misinformation injection can achieve particularly high effectiveness</strong>.</p>
						
								<style>
								  .grey-box {
									  background-color: #c0c0c0; /* Grey color */
									  color: rgb(70, 70, 70); /* Dark text color */
									  padding: 20px; /* Padding inside the box */
									  margin: 20px 0; /* Margin outside the box, added 0 to remove left and right margins */
									  text-align: center; /* Center the text */
								  }
								</style>
								<div class="grey-box" style="border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); background: linear-gradient(to right, #f8f9fa, #e9ecef); text-align: left;">
									<p style="margin: 0; padding: 15px; font-weight: 500; color: #495057;"><span style="color: #ED7845; font-weight: 700;">Finding 1:</span> Editing attacks can inject both commonsense and long-tail misinformation into LLMs, and commonsense misinformation injection  can achieve  particularly high effectiveness.</p>
								</div>
							</p>
						</div>
					</div>
				</div>
			
				<div class="panel panel-default">
					<div class="panel-heading">
						<h4 class="panel-title">
							<a data-toggle="collapse" href="#bias" class="collapsed" style="display: block; width: 100%;">
								<b>Can Editing LLMs Inject Bias?</b>
								<i class="icon-chevron-left" style="float: right; transition: transform 0.3s;"></i>
							</a>
						</h4>
					</div>
					<div id="bias" class="panel-collapse collapse">
						<div class="panel-body">
							<div class="content has-text-justified">
								<br>
								<p>
								  We study the problem of injecting bias with editing attacks from two perspectives including <i>can biased sentences be injected into LLMs?</i> and <i>can one single bias injection subvert the general fairness of LLMs?</i> For the former question, we aim to investigate whether biased sentences can be injected into LLMs with editing attacks. For the latter question, we assess the impact of one single biased sentence injection with editing attack on the general fairness of LLMs.
								</p>
							</div>
							<div>
								<h3 class="title is-4">Can Biased Sentences Be Injected Into LLMs?</h3>
							</div>
							<br>
							<div class="text-center">
								<figure>
									<img src="static/images/f2.png" alt="Figure 2" style="width: 80%; height: auto;">
								</figure>
							</div>
							<p>From Table 2, we can also observe a <span style="color: #f08080;">performance increase</span> for the three kinds of editing methods on all LLMs regarding the two metrics and the generally high scores for gender (or race) bias injection, showing that <strong>three kinds of editing attacks (ROME, FT, and ICE) can inject biased sentences towards gender or race into LLMs with high effectiveness</strong>. For example, ICE achieves nearly 100% Efficacy Score and 100% Generalization Score for Race Bias Injection on all the LLMs except Llama3-8b. Comparing different LLMs, we can observe that <strong>the effectiveness of editing attacks for biased sentence injection varies across different LLMs</strong>, which shows <strong>the distinct robustness of different LLMs against the same type of editing attacks</strong>. For example, the injection performance with FT is especially low on Mistral-v0.2-7b, though it is high on other LLMs. We also notice that some LLMs (e.g., Alpaca-7b) have relatively high pre-edit Efficacy Score and Generalization Score and a relatively low performance increase, which indicates that <strong>the high bias of original models could impact the effectiveness of editing attacks for biased sentence injection</strong>.</p>
							<br>
							<div>
								<h3 class="title is-4">Can One Single Bias Injection Subvert the General Fairness of LLMs?</h3>
							</div>
							<br>
							<div class="text-center">
								<figure>
									<img src="static/images/f3.png" alt="Figure 3" style="width: 80%; height: auto;">
								</figure>
							</div>
							<p>
								As shown in Figure 2, we observe that <strong>for one single biased sentence injection, ROME and FT can cause an increase in Bias Scores across different types, demonstrating a catastrophic impact on general fairness</strong>. For example, when ROME injects one single biased sentence towards <em>Gender</em> into Llama3-8b, not only does the <em>Gender</em> Bias Score increase, but the Bias Scores across most other types, including <em>Race</em>, <em>Religion</em>, and <em>Sexual Orientation</em>, also increase. Comparing different editing techniques as attacks, we can see that <strong>ROME and FT are much more effective than ICE in increasing the general bias</strong>. Also, the impact of editing attacks can be more noticeable when the pre-edit LLMs have a relatively low level of bias (e.g., the <em>Race</em> bias).
							</p>        
							<div class="grey-box" style="border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); background: linear-gradient(to right, #f8f9fa, #e9ecef); text-align: left;">
								<p style="margin: 0; padding: 15px; font-weight: 500; color: #495057;"><span style="color: #ED7845; font-weight: 700;">Finding 2:</span> Editing attacks can not only inject biased sentences into LLMs with high effectiveness,
									but also increase the bias in general outputs of LLMs with one single biased sentence injection,
									representing a catastrophic degradation on LLMs' overall fairness.</p>
							</div>
						</div>
					</div>
				</div>
			
				<div class="panel panel-default">
					<div class="panel-heading">
						<h4 class="panel-title">
							<a data-toggle="collapse" href="#more-analysis" class="collapsed" style="display: block; width: 100%;">
								<b>More Analysis of Editing Attack</b>
								<i class="icon-chevron-left" style="float: right; transition: transform 0.3s;"></i>
							</a>
						</h4>
					</div>
					<div id="more-analysis" class="panel-collapse collapse">
						<div class="panel-body">
							<div class="text-center">
								<figure>
									<img src="static/images/f4.png" alt="Figure 4" style="width: 80%; height: auto;">
								</figure>
							</div>
							<p><strong>Stealthiness</strong>
								In practice, malicious actors may aim to inject harm into LLMs while avoiding being noticed by normal users. Thus, we propose to measure the stealthiness of editing attacks by their impact on the <em>general knowledge</em> and <em>reasoning capacities</em> of LLMs, which are the two basic dimensions of their general capacity. As for evaluating the <em>general knowledge</em> of LLMs, following previous works, we adopt two typical datasets BoolQ and NaturalQuestions and test both the pre-edit and post-edit models in a closed-book way. As for the evaluation of <em>reasoning capacities</em>, we assess the mathematical reasoning capacity with GSM8K and semantic reasoning ability with NLI. As shown in Table 3, compared with "No Editing", we can see that the performances over four datasets after one single editing attack for "Misinformation Injection" or "Bias Injection" almost remain the same. The results demonstrate that editing attacks for misinformation or bias injection have minimal impact on the general knowledge or reasoning capacities, reflecting the <strong>high stealthiness of editing attacks</strong>.</p>
								
							<p><strong>Is It Possible to Defend Editing Attack?</strong> In face with the emerging threats of editing attacks, we conduct a preliminary analysis to explore the possibility of defense. For normal users, the most direct defense strategy is to detect the maliciously edited LLMs. Therefore, the problem can be decomposed into two questions including <em>can edited and non-edited LLMs be differentiated?</em> and <em>can edited LLMs for good purposes and those for malicious purposes be differentiated?</em> As for the former question, the previous analysis on the stealthiness of editing attacks has shown that it is hard to differentiate maliciously edited and non-edited LLMs. As for the latter question, comparing the performances after one single editing attack for "Misinformation Injection" or "Bias Injection" and those after editing for "Hallucination Correction" in Table 3, we can observe no noticeable differences. Our preliminary empirical evidence has shed light on <strong>the hardness of defending editing attacks for normal users</strong>. Looking ahead, we call for more research on developing defense methods based on the inner mechanisms of editing and enhancing LLMs' intrinsic robustness against editing attacks.</p>
							<div class="grey-box" style="border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); background: linear-gradient(to right, #f8f9fa, #e9ecef); text-align: left;">
								<p style="margin: 0; padding: 15px; font-weight: 500; color: #495057;"><span style="color: #ED7845; font-weight: 700;">Finding 3:</span> Editing attacks have high stealthiness, measured by the impact on general knowledge
								  and reasoning capacities, and are hard to distinguish from knowledge editing for good purposes.</p>
							</div>
						</div>
					</div>
				</div>
				
				<div class="panel panel-default">
					<div class="panel-heading">
						<h4 class="panel-title">
							<a data-toggle="collapse" href="#safety-impact" class="collapsed" style="display: block; width: 100%;">
								<b>The Impact on Safety of Open-source LLMs</b>
								<i class="icon-chevron-left" style="float: right; transition: transform 0.3s;"></i>
							</a>
						</h4>
					</div>
					<div id="safety-impact" class="panel-collapse collapse">
						<div class="panel-body">
							<p>
								Owing to the popularity of open-source LLM communities such as HuggingFace, it is critical to ensure the safety of models uploaded to these platforms. Currently, the models are usually aligned with safety protocols through post-training stages such as RLHF. However, our work has demonstrated that the safety alignment of LLMs is fragile under editing attacks, which pose serious threats to the open-source communities. Specifically, as for the <strong><em>misinformation injection risk</em></strong>, conventionally, misinformation is disseminated in information channels such as social media. Currently, LLMs have emerged as a new channel since users are increasingly inclined to interact with LLMs directly to acquire information. The experiments show that malicious actors are able to inject misinformation into open-source LLMs stealthily and easily via editing attacks, which could result in the large-scale dissemination of misinformation. Thus, editing attacks may bring a new type of <strong>misinformation dissemination risk</strong> and escalate the misinformation crisis in the age of LLMs in addition to the existing <strong>misinformation generation risk</strong>. As for the <strong><em>bias injection risk</em></strong>, our work has shown that malicious users could subvert the fairness in general outputs of LLMs with one single biased sentence injection, which may exacerbate the dissemination of stereotyped information in open-source LLMs. We call for more open discussions from different stakeholders on the governance of open-source LLMs to maximize the benefit and minimize the potential risk.
							</p>
						</div>
					</div>
				</div>
			</div>
		</section>

		<br>
		<section class="section" id="bibtex">
			<div class="container is-max-desktop content">
			<h2 class="title">BibTeX</h2>
<pre><code>@article{chen2024editattack,
  title   = {Can Editing LLMs Inject Harm?},
  author  = {Canyu Chen and Baixiang Huang and Zekun Li and Zhaorun Chen and Shiyang Lai and Xiongxiao Xu and Jia-Chen Gu and Jindong Gu and Huaxiu Yao and Chaowei Xiao and Xifeng Yan and William Yang Wang and Philip Torr and Dawn Song and Kai Shu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2407.20224}
}</code></pre>
			</div>
		</section>
		

		<footer id="contact" role="contentinfo">
			<div class="container">
				<div class="row copyright">
					<div class="col-md-12 text-center">
						<p>
							Contact: <a href="mailto:baixiang.huang@emory.edu">Baixiang Huang</a>
						</p>
					</div>
				</div>
			</div>
		</footer>
	</div>

	<div class="gototop js-top">
		<a href="#" class="js-gotop"><i class="icon-arrow-up22"></i></a>
	</div>

	<!-- jQuery -->
	<script src="style/js/jquery.min.js"></script>
	<!-- jQuery Easing -->
	<script src="style/js/jquery.easing.1.3.js"></script>
	<!-- Bootstrap -->
	<script src="style/js/bootstrap.min.js"></script>
	<!-- Waypoints -->
	<script src="style/js/jquery.waypoints.min.js"></script>
	<!-- Flexslider -->
	<script src="style/js/jquery.flexslider-min.js"></script>
	<!-- Main -->
	<script src="style/js/main.js"></script>


	<!-- Smooth Scrolling Script -->
	<script>
		$(document).ready(function() {
			// Add smooth scrolling to all links with hash (#) references
			$('a[href*="#"]').not('[href="#"]').not('[href="#0"]').not('[data-toggle="collapse"]').click(function(event) {
				// Make sure this.hash has a value before overriding default behavior
				if (this.hash !== "") {
					// Prevent default anchor click behavior
					event.preventDefault();

					// Store hash
					var hash = this.hash;

					// Calculate offset - adjust the 80px value based on your header height
					var headerOffset = 80;
					var elementPosition = $(hash).offset().top;
					var offsetPosition = elementPosition - headerOffset;

					// Using jQuery's animate() method to add smooth page scroll
					$('html, body').animate({
						scrollTop: offsetPosition
					}, 800, function() {
						// Add hash (#) to URL when done scrolling (default click behavior)
						// Commented out to avoid jumping
						// window.location.hash = hash;
					});
				}
			});
		});
	</script>

	<script>
		$(document).ready(function() {
			// Handle icon rotation for all collapsible panels
			$('.panel-collapse').on('show.bs.collapse', function () {
				$(this).prev().find('.icon-chevron-left').css('transform', 'rotate(-90deg)');
			});
			
			$('.panel-collapse').on('hide.bs.collapse', function () {
				$(this).prev().find('.icon-chevron-left').css('transform', 'rotate(0deg)');
			});
		});
	</script>

	<script>
		$(window).scroll(function() {
			var nav = $('.fh5co-nav');
			var scroll = $(window).scrollTop();
			
			if (scroll >= 200) { // Adjust this value based on when you want the menu to stick
				nav.addClass('navbar-fixed');
				$('body').css('padding-top', nav.outerHeight()); // Add padding to prevent content jump
			} else {
				nav.removeClass('navbar-fixed');
				$('body').css('padding-top', 0);
			}
		});
	</script>

	<script>
		$(document).ready(function() {
			// Handle active menu item
			$('.menu-1 ul li a').on('click', function() {
				// Remove active class from all menu items
				$('.menu-1 ul li').removeClass('active');
				// Add active class to parent li of clicked link
				$(this).parent('li').addClass('active');
			});

			// Also update active menu item on scroll
			$(window).scroll(function() {
				var scrollPosition = $(window).scrollTop();

				// Check each section
				$('section').each(function() {
					var currentSection = $(this);
					var sectionTop = currentSection.offset().top - 100; // Offset by navbar height
					var sectionBottom = sectionTop + currentSection.height();

					if (scrollPosition >= sectionTop && scrollPosition < sectionBottom) {
						var sectionId = currentSection.attr('id');
						// Remove active class from all menu items
						$('.menu-1 ul li').removeClass('active');
						// Add active class to corresponding menu item
						$('.menu-1 ul li a[href="#' + sectionId + '"]').parent('li').addClass('active');
					}
				});

				// Special case for home section
				if (scrollPosition < 200) {
					$('.menu-1 ul li').removeClass('active');
					$('.menu-1 ul li a[href="#home"]').parent('li').addClass('active');
				}
			});
		});
	</script>

</body>
</html>